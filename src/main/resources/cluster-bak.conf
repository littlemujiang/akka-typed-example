akka {

    # JVM shutdown, System.exit(-1), in case of a fatal error,
    # such as OutOfMemoryError
    jvm-exit-on-fatal-error = off
    loglevel = "INFO"
    loggers = ["akka.event.slf4j.Slf4jLogger"]
#    loggers = ["akka.event.Logging$DefaultLogger"]

    actor {
#             provider = "akka.cluster.ClusterActorRefProvider"
             provider = "cluster"
#             allow-java-serialization = on
#             warn-about-java-serializer-usage = off
             serializers {
                jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
#                jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
#                proto = "akka.remote.serialization.ProtobufSerializer"
#                myown = "docs.serialization.MyOwnSerializer"
             }
            serialization-bindings {
                "com.example.akka.message.DevicePropertyMessage" = jackson-json
            }
    }

#    remote {
##        enabled-transports = ["akka.remote.netty.tcp"]
#        log-remote-lifecycle-events = off
#        use-passive-connections = off
#        netty.tcp {
#            hostname = "127.0.0.1"
#            port= 5001
#        }
#
#    }
   remote.artery {
        canonical {
        hostname = "127.0.0.1"
        port = 5001
   }
}

    cluster {
        seed-nodes = [
                "akka://cluster-example@127.0.0.1:5001",
                "akka://cluster-example@127.0.0.1:5002"
#                "akka.tcp://cluster-mvp@127.0.0.1:5003"
        ]
#        downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
#        split-brain-resolver.active-strategy = keep-oldest
        auto-down-unreachable-after = off
#        roles = ["demo"]
        distributed-data {
        # Actor name of the Replicator actor, /system/ddataReplicator
                name = ddataReplicator
        # Replicas are running on members tagged with this role.
        # All members are used if undefined or empty.
                role = ""
        # How often the Replicator should send out gossip information
                gossip-interval = 2 s
        # How often the subscribers will be notified of changes, if any
                notify-subscribers-interval = 500 ms
        # The id of the dispatcher to use for Replicator actors.
        # If specified you need to define the settings of the actual dispatcher.
                use-dispatcher = "akka.actor.internal-dispatcher"
        # How often the Replicator checks for pruning of data associated with
        # removed cluster nodes. If this is set to 'off' the pruning feature will
        # be completely disabled.
                pruning-interval = 120 s
        }
#        singleton {
#        # The actor name of the child singleton actor.
#                singleton-name = "GlobalFilter"
#
#        # Singleton among the nodes tagged with specified role.
#        # If the role is not specified it's a singleton among all nodes in the cluster.
#                role = ""
#
#        # When a node is becoming oldest it sends hand-over request to previous oldest,
#        # that might be leaving the cluster. This is retried with this interval until
#        # the previous oldest confirms that the hand over has started or the previous
#        # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
#                hand-over-retry-interval = 1s
#                removalMargin = 1s
#        # The number of retries are derived from hand-over-retry-interval and
#        # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
#        # but it will never be less than this property.
#        # After the hand over retries and it's still not able to exchange the hand over messages
#        # with the previous oldest it will restart itself by throwing ClusterSingletonManagerIsStuck,
#        # to start from a clean state. After that it will still not start the singleton instance
#        # until the previous oldest node has been removed from the cluster.
#        # On the other side, on the previous oldest node, the same number of retries - 3 are used
#        # and after that the singleton instance is stopped.
#        # For large clusters it might be necessary to increase this to avoid too early timeouts while
#        # gossip dissemination of the Leaving to Exiting phase occurs. For normal leaving scenarios
#        # it will not be a quicker hand over by reducing this value, but in extreme failure scenarios
#        # the recovery might be faster.
#                min-number-of-hand-over-retries = 15
#        }
        singleton-proxy {
        # The actor name of the singleton actor that is started by the ClusterSingletonManager
                singleton-name = "GlobalFilter"

        # The role of the cluster nodes where the singleton can be deployed.
        # If the role is not specified then any node will do.
                role = ""

        # Interval at which the proxy will try to resolve the singleton instance.
                singleton-identification-interval = 1s

        # If the location of the singleton is unknown the proxy will buffer this
        # number of messages and deliver them when the singleton is identified.
        # When the buffer is full old messages will be dropped when new messages are
        # sent via the proxy.
        # Use 0 to disable buffering, i.e. messages will be dropped immediately if
        # the location of the singleton is unknown.
        # Maximum allowed buffer size is 10000.
                buffer-size = 1000
        }
    }
}

# This dispatcher is used for rule node actor
my-dispatcher {
        type = Dispatcher
        executor = "fork-join-executor"
        fork-join-executor {
# Min number of threads to cap factor-based parallelism number to
        parallelism-min = 1
# Max number of threads to cap factor-based parallelism number to
        parallelism-max = 50

# The parallelism factor is used to determine thread pool size using the
# following formula: ceil(available processors * factor). Resulting size
# is then bounded by the parallelism-min and parallelism-max values.
        parallelism-factor = 3.0
}
# How long time the dispatcher will wait for new actors until it shuts down
        shutdown-timeout = 60s

# Throughput defines the number of messages that are processed in a batch
# before the thread is returned to the pool. Set to 1 for as fair as possible.
        throughput = 5
}



